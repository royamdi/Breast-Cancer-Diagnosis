# -*- coding: utf-8 -*-
"""DataMiningProject3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WAJVUELvdZgkIbOJFrmdJ3_H4aBrWdyJ
"""

from sklearn import datasets
import pandas as pd
import numpy as np
pd.set_option('display.max_columns', 50)
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go

"""# Loading The Data

"""

# load raw train data
!gdown 1EGHpyOZL8RnOPnz8WY-OGDMi-rSBC8I5
df = pd.read_csv('/content/breast cancer.csv')
df.head()

"""در صورت وجود داده از ست رفته آن را باید حذف کنیم."""

# checking for missing values
print(df.isna().any())

"""برای راحتتر کردن بررسی ستون‌ها را به حالت عمودی تبدیل میکنیم. زیرا تعداد ستون‌ها زیاد است.
و همچنین از دستور دیسکرایب برای تحلیل آماری استفاده میکنیم.


"""

df.describe().transpose()

"""دیتاها را به دو حالت بدخیم و خوش خیم تقسیم میکنیم و به خوش خیم‌ها 1 و به بدخیم‌ها 0 نسبت میدهیم."""

df['diagnosis'] = df['diagnosis'].replace({'M': 1, 'B': 0})

"""چک میکنیم که در دیتاها، دیتای دابلیکیت و تکراری وجود دارد یا خیر. اگر بود آن را حذف میکنیم."""

print(df[df.duplicated()])

"""unnamed , id
حذف میشوند چون ارزش اطلاعاتی زیادی ندارند.
"""

df.drop(['Unnamed: 32','id'], axis = 1,inplace=True)

"""تارگت را تعیین میکنیم و آنها را رسم میکنیم تا توازن آنها را ببینیم."""

# target distribution
sns.countplot(x=df['diagnosis'])

"""مارتیس کورلیشن را رسم میکنیم تا بتوانیم بفهمیم کدامین از دیتاها بیشتر از همه باهم همبستگی دارند و روی هم تاثیر میگذارند."""

# correlation matrix
fig = plt.figure(figsize=(20,15))
sns.heatmap(df.corr(), cmap='coolwarm', annot=True, fmt=format('.2f'), linewidths=0.003)

"""# Feature Selection with 0.90 Threshold

در اینجا دیتاهایی که بالای 90 درصد همبستگی دارند را رسم میکند. در سمت راست
پایین سه داده داریم که با 0.99 , 0.98 , 0.98 همبستگی دارند و از طریق همدیگر به دست می آیند. پس دوتای آنها را حذف و یکی از آنها را نگه میداریم.
"""

# remove highly correlated features
correlation_mat = df.corr()
for row in correlation_mat.index:
    for col in correlation_mat.columns:
        if row==col or correlation_mat.loc[row,col].round(2)<0.90:
            correlation_mat.loc[row,col] = np.nan
fig = plt.figure(figsize=(12,10))
highly_correlated = correlation_mat.dropna(how='all').dropna(axis=1,how='all')
mask = np.tril(np.ones_like(highly_correlated))
sns.heatmap(highly_correlated, cmap='coolwarm', mask=mask,annot=True, fmt=format('.2f'), linewidths=0.003)
plt.title('Highly Correlated Features')

"""دوباره نمودار را میکشیم و میبینیم که مشکل دیتاهایی که همبستگی زیادی داشتند برطرف شدند."""

# Manual Feature Extraction Results

df_1 = df.drop(columns=['perimeter_mean', 'area_mean', 'perimeter_se', 'perimeter_worst', 'area_worst','radius_worst', 'texture_worst', 'area_se', 'concave points_worst', 'concavity_mean'])
correlation_mat = df_1.corr()
fig = plt.figure(figsize=(20,15))
sns.heatmap(correlation_mat, cmap='coolwarm',annot=True, fmt=format('.2f'), linewidths=0.003)
plt.title('Modified Correlation')

"""همبستگی دیتاها را با ویژگی هدف بررسی میکنیم که ببینیم چقدر بقیه ویژگی‌ها روی آن تاثیر میگذراند. میبینیم که همه آنها تاثیر مستقیمی دارند."""

# correlation of features with target
target_corr = correlation_mat['diagnosis'].drop(['diagnosis'])
target_corr.plot(kind='bar', figsize=(15,8))

"""نمودار kde
این نمودار یک نمایش توزیع احتمالی است و درواقع تابع تراکمی احتمال است.
 احتمال تراکم هر ویژگی‌ای را بازگو میکند و نشان میدهد.
"""

# kde plot for features
plt.figure(figsize = (14, 20))
for i,c in enumerate(df_1.columns):
    if c != 'diagnosis':
        ax = plt.subplot(10, 3, i+1)
        sns.histplot(df_1[c], kde=True, stat='density', linewidth=0)
        plt.xlabel(c)

plt.tight_layout()
plt.show()

"""##Split

ستون هدف را دراپ میکنیم  و با نسبت سایز 0.2 بقیه ویژگی‌ها را تقسیم میکنیم.

بااستفاده از train_test_split

استیت رندوم هم عددی گذاشتیم که هرموقع از این برنامه استفاده کردیم جواب یکسانی به ما بدهد.
"""

from sklearn.model_selection import train_test_split

X = df_1.drop(columns=['diagnosis'])
Y = df_1['diagnosis']
x_train , x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=10)

"""در اینجا دیتاها را استانداردسازی میکنیم با استفاده از استاندارد اسکالر.

بعد از استانداردسازی با استفاده از الگوریتم پی سی ای مولفه‌های اصلی را استخراج میکنیم. سپس داده ها را نرمال سازی میکنیم.
"""

# standardizing features
from sklearn.preprocessing import StandardScaler, Normalizer
from sklearn.decomposition import PCA

# separating target from the rest

scaler = StandardScaler()
Fit_std = scaler.fit(x_train)
x_train_std = Fit_std.transform(x_train)
x_test_std = Fit_std.transform(x_test)

# # dimension reduction using pca
pca = PCA(n_components='mle')
Fit_pca = pca.fit(x_train_std)
x_train_pca = Fit_std.transform(x_train_std)
x_test_pca = Fit_std.transform(x_test_std)

norm = Normalizer()
Fit_nrm = norm.fit(x_train_pca)
x_train_pca= Fit_nrm.transform(x_train_pca)
x_test_pca= Fit_nrm.transform(x_test_pca)


pd.DataFrame(x_train_pca).describe().transpose()

"""در اینجا دوباره از نمودار ذکر شده استفاده میکنیم و میبینیم که توزیع ویژگی‌ها و داده‌ها بهتر و به نرمال نزدیک می‌شوند."""

# kde plot for features after standardization and normalization
x_train_pca_df = pd.DataFrame(x_train_pca)
plt.figure(figsize = (14, 20))
for i,c in enumerate(x_train_pca_df.columns):
    if c != 'diagnosis':
        ax = plt.subplot(10, 3, i+1)
        sns.histplot(x_train_pca_df[c], kde=True, stat='density', linewidth=0)
        plt.xlabel(c)

plt.tight_layout()
plt.show()

"""---

# Modeling

برای مدلسازی از کتابخانه تنسورفلو با رابط کراس استفاده میکنیم.
به دلیل اینکه ابعاد به 20 مولفه اصلی کاهش پیدا کرده است، لایه اول شبکه عصبی را با 20 نورون قرار دادیم با تابع فعال ساز ذکر شده، لایه دوم را با ۷ نورون و لایه سوم ۴ نورون ساخته شده است. سپس لایه خروجی دارای دو نورون با تابع فعالسازی سیگموید برای نمایش خروجی بصورت احتمالاتی ساخته شده است. دلیل انتخاب تعداد لایه های پنهان سعی و خطا و دلیل انتخاب تعداد نورون های هر لایه پنهان معیار میانگین هندسی تعداد نورون های لایه قبل و بعد آن است.

از اپتیمایزر های آدام و گرادیان کاهشی استوک استیک استفاده کرده ایم و آموزش مدل را به تابعی سپرده ایم که ورودی آن اپتیمایزر دلخواه، تعداد ایپاک ها و بچ سایز دلخواه است. سپس مدل های متعددی را ساخته و با استفاده از این تابع اموزش داده ایم. ابتدا تعیین لرنینگ ریت را به عهده خود اپتیمایزر قرار دادیم تا بتوانیم تاثیر ایپاک و بچ سایز را در خروجی مدل بسنجیم.
"""

import tensorflow as tf
tf.random.set_seed(3)
from keras.models import Sequential
from keras import layers,optimizers


def process_model(optimizer, epoch, batch_size, vb=0):
  model = Sequential([
                      layers.Dense(20),
                      layers.Dense(7, activation='relu'),
                      layers.Dense(4, activation='relu'),
                      layers.Dense(2, activation='sigmoid')
  ])
  if isinstance(optimizer, str):
    opt_label = optimizer
  else:
    opt_label  = str(optimizer.get_config()['name'])
    opt_label += '(' + str(optimizer.get_config()['learning_rate']) + ')'
  model.compile(optimizer=optimizer, loss=tf.keras.losses.mean_absolute_error, metrics=['accuracy'])
  history = model.fit(x_train_pca_df, y_train, epochs=epoch, batch_size=batch_size, validation_split = 0.2, verbose=vb)
  sns.lineplot(history.history['accuracy'], label=f'OPT:{opt_label} - EP:{epoch} - BC:{batch_size} ({history.history["accuracy"][-1]:.3%})')
  return model

models = []
# MODELS =======================================================================
fig = plt.figure(figsize=(15,7))
models.extend([
  process_model('adam', 10, 1),
  process_model('adam', 10, x_train_pca_df.shape[0]),
  process_model('adam', 10, x_train_pca_df.shape[0]//10),

  process_model('adam', 50, 1),
  process_model('adam', 50, x_train_pca_df.shape[0]),
  process_model('adam', 50, x_train_pca_df.shape[0]//10),
])
plt.xlabel('Epochs')
plt.ylabel('Acc')
plt.title('Models with adaptive learning rate ADAM')
fig.show()
# ==============================================================================
fig = plt.figure(figsize=(15,7))
models.extend([
  process_model('sgd', 10, 1),
  process_model('sgd', 10, x_train_pca_df.shape[0]),
  process_model('sgd', 10, x_train_pca_df.shape[0]//10),

  process_model('sgd', 20, 1),
  process_model('sgd', 20, x_train_pca_df.shape[0]),
  process_model('sgd', 20, x_train_pca_df.shape[0]//10),
])
plt.xlabel('Epochs')
plt.ylabel('Acc')
plt.title('Models with adaptive learning rate SGD')
fig.show()
# ==============================================================================

"""سپس با توجه به عملکرد هر مدل، از بین آنها بهترین ها را انتخاب کرده و تاثیر لرنینگ ریت را روی آنها میسنجیم. همانطور که مشخص است لرنینگ ریت وابسته به ایپاک و بچ سایز و همچنین خود اپتیمایزر مورد استفاده است. در این میان متوجه شدیم برای اپتیمایزر آدام لرنینگ ریت های کمتر و برای اپتیمایزر گرادیان کاهشی استوک استیک لرنینگ ریت بیشتر نتیجه بهتری میدهد. همچنین برای اپتیمایزر آدام ایپاک بالاتر و بچ سایز بیشتر عملکرد آن را بهبود میبخشد."""

fig = plt.figure(figsize=(15,7))
optimizer = optimizers.Adam(learning_rate=0.1)
models.extend([
  process_model(optimizer, 10, 1),
  process_model(optimizer, 50, 1),
  process_model(optimizer, 50, x_train_pca_df.shape[0]//10),
])
optimizer = optimizers.Adam(learning_rate=0.001)
models.extend([
  process_model(optimizer, 10, 1),
  process_model(optimizer, 50, 1),
  process_model(optimizer, 50, x_train_pca_df.shape[0]//10),
])
plt.xlabel('Epochs')
plt.ylabel('Acc')
plt.title('Models with manual learning rate ADAM')
fig.show()

fig = plt.figure(figsize=(15,7))
optimizer = optimizers.SGD(learning_rate=0.1)
models.extend([
  process_model(optimizer, 20, 1),
])
optimizer = optimizers.SGD(learning_rate=0.001)
models.extend([
  process_model(optimizer, 20, 1),
])

plt.xlabel('Epochs')
plt.ylabel('Acc')
plt.title('Models with manual learning rate SGD')
fig.show()

# from keras import backend as K
# change learning rate
# K.set_value(model.optimizer.learning_rate, 0.001)
# print("Learning rate before second fit:", model.optimizer.learning_rate.numpy())
# history = model.fit(x_train_pca_df, y_train, epochs=100, batch_size=32, validation_split = 0.2)

"""#Evaluation

در انتها برای بهترین مدل اموزش داده شده ماتریس درهم ریختگی و معیار های ارزیابی پرسیژن و ریکال را محاسبه میکنیم.
"""

from sklearn.metrics import precision_score,recall_score,f1_score,confusion_matrix,ConfusionMatrixDisplay
import numpy as np

def performance_measure(model):
  # predict probabilities for test set
  yhat_probs = model.predict(x_test_pca, verbose=0)
  # predict crisp classes for test set
  yhat_classes = np.argmax(yhat_probs,axis=1)

  # precision tp / (tp + fp)
  precision = precision_score(y_test, yhat_classes)
  print('Precision: %f' % precision)
  # recall: tp / (tp + fn)
  recall = recall_score(y_test, yhat_classes)
  print('Recall: %f' % recall)
  # f1: 2 tp / (2 tp + fp + fn)
  f1 = f1_score(y_test, yhat_classes)
  print('F1 score: %f' % f1)
  #draw confusion matrix
  cm = confusion_matrix(y_test, yhat_classes)
  disp = ConfusionMatrixDisplay(confusion_matrix=cm)
  disp.plot()
  plt.show()

performance_measure(models[17])
# print(f'The evaluation of {clf} model')
# predictions = clf.predict(X_test)
# #Classification report
# print(classification_report(Y_test, predictions ,digits = 4))

"""---
بطور کلی مشاهده میشود که در بیشتر مدل های نا دقیق مشکل اصلی بایاس بودن مدل به یک کلاس است. یعنی تمام داده های تست را در یک کلاس طبقه بندی میکند. به همین دلیل ممکن است بتوان با تغییر در فرایند پیش پردازش داده ها و یا تغییر ساختار شبکه عصبی این بایاس بودن را کاهش داد تا به نتایج بهتری دست پیدا کرد.

جهت بررسی بیشتر تمام ماتریس های درهم ریختگی برای تمام مدل ها در زیر آمده است.
"""

for model in models:
  print('Optimizer:',model.optimizer.get_config()['name'],'\tlearning rate:',model.optimizer.get_config()['learning_rate'])
  performance_measure(model)

